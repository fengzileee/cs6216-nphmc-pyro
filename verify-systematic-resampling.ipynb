{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "059f62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "from pyro.infer.importance import Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c179b02",
   "metadata": {},
   "source": [
    "# Whatsapp puzzle\n",
    "\n",
    "Probability of a day being Sunday is givn by Bernoulli distribution\n",
    "$$P(A) = p_{A}(x) = \\mathrm{Bern}\\left(x;1/7\\right)$$\n",
    "\n",
    "If the day is Sunday, number of messages $B$ per hour is given by the Poisson distribution\n",
    "$$P(B|A=1) = p_{B}(x|A=1) = \\mathrm{Poi}(x;3)$$\n",
    "otherwise\n",
    "$$P(B|A=0) = p_{B}(x|A=0) = \\mathrm{Poi}(x;10)$$\n",
    "\n",
    "If receives $4$ messages on a day, what is the probability that the day is Sunday\n",
    "$$P(A=1|B=4)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd354c88",
   "metadata": {},
   "source": [
    "# Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ede979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale():\n",
    "    weight = pyro.sample(\"weight\", pyro.distributions.Uniform(0.0, 3.0), is_cont=False)\n",
    "    if weight < 2.2:\n",
    "        m = pyro.sample(\"measurement_1\", pyro.distributions.Normal(weight - 1, 0.75), is_cont=False)\n",
    "    else:\n",
    "        tmp = pyro.sample(\"measurement_2\", pyro.distributions.Normal(weight + 1, 0.75), is_cont=False)\n",
    "        m = pyro.sample(\"measurement_1\", pyro.distributions.Normal(tmp + 1, 0.25), is_cont=False)\n",
    "    pyro.sample(\"obs\", pyro.distributions.Normal(3.1, 0.25), obs=m)\n",
    "    return weight.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c6437db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyro.infer.importance.Importance at 0x7fd1650d8ac8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = Importance(scale, guide=None, num_samples=20000)\n",
    "importance.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "227b924a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4446210441454292"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_weights = np.array([w.item() for w in importance.log_weights])\n",
    "values = np.array([t.nodes['weight']['value'].item() for t in importance.exec_traces])\n",
    "weights = np.exp(log_weights)\n",
    "np.sum(weights * values) / np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff6395",
   "metadata": {},
   "source": [
    "# Sample from posterior via systematic resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6906813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def systematic_resampling(log_weights, values):\n",
    "    import torch\n",
    "\n",
    "    mx = max(log_weights)\n",
    "    weight_sum = sum(math.exp(log_weight - mx) for log_weight in log_weights)\n",
    "    u_n = torch.distributions.Uniform(0, 1).sample().item()\n",
    "    sum_acc = 0.0\n",
    "    resamples = []\n",
    "    for (log_weight, value) in zip(log_weights, values):\n",
    "        weight = math.exp(log_weight - mx) * len(values) / weight_sum\n",
    "        sum_acc += weight\n",
    "        while u_n < sum_acc:\n",
    "            u_n += 1\n",
    "            resamples.append(value)\n",
    "    return resamples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bec4c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.445070327234268\n"
     ]
    }
   ],
   "source": [
    "resamples = systematic_resampling(log_weights, values)                   \n",
    "print(np.mean(resamples))                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b5a8f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 1100/1100 [08:11,  2.24it/s, step size=1.00e-01, acc. prob=0.551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weight': torch.Size([]), 'measurement_2': torch.Size([]), 'measurement_1': torch.Size([])}\n",
      "weight torch.Size([682]) tensor([2.7067, 2.5067, 2.3067, 2.7067, 2.1067, 2.1454, 2.1454, 2.1454, 2.5454,\n",
      "        2.5454, 2.5454, 2.7822, 2.2350, 2.4350, 2.4350, 2.4115, 2.2115, 1.5362,\n",
      "        1.5362, 1.7362, 1.9362, 2.9556, 2.4638, 2.4074, 1.4074, 2.0290, 2.0290,\n",
      "        2.8281, 1.6414, 1.6414, 1.6414, 1.6414, 1.6414, 1.6414, 1.8414, 1.8414,\n",
      "        3.0414, 3.9952, 1.7952, 2.3297, 1.9297, 1.9297, 1.9297, 2.1297, 2.3297,\n",
      "        2.3098, 2.3098, 2.7707, 2.7707, 2.7502, 2.9389, 2.3764, 2.3764, 2.1764,\n",
      "        2.1764, 2.5764, 3.1764, 3.1764, 3.1764, 3.1764, 3.3764, 3.3764, 2.5764,\n",
      "        1.7764, 2.1764, 2.7764, 2.9764, 2.9764, 1.7659, 1.7659, 2.1659, 2.1659,\n",
      "        1.9554, 1.9554, 2.7554, 2.7554, 2.7554, 2.7554, 2.9692, 2.5339, 3.1339,\n",
      "        3.1339, 3.1339, 2.5339, 2.5339, 2.7353, 2.7353, 3.1353, 3.3353, 1.8610,\n",
      "        2.6610, 2.3398, 2.3398, 2.1398, 2.1398, 2.9398, 3.3185, 3.3185, 3.2686,\n",
      "        3.0686, 3.0686, 2.2866, 2.2866, 1.6540, 1.6540, 1.6540, 2.0540, 1.6575,\n",
      "        1.6575, 2.2944, 2.2944, 1.8940, 1.8940, 1.8940, 2.4940, 1.6940, 1.6940,\n",
      "        2.1134, 1.8932, 2.8932, 2.8932, 1.9770, 2.3770, 2.9770, 2.7461, 2.7461,\n",
      "        2.7461, 2.5461, 3.1461, 2.9017, 2.7017, 2.3017, 2.7017, 2.8023, 2.8023,\n",
      "        2.0023, 2.0023, 2.0023, 2.4529, 2.0529, 3.0529, 2.4529, 2.6138, 2.6138,\n",
      "        2.6138, 2.6138, 2.1818, 1.7275, 2.7275, 2.3275, 2.6058, 2.7254, 2.7254,\n",
      "        2.8212, 2.0212, 2.0212, 2.0212, 2.0212, 1.8212, 2.6474, 2.6474, 2.6474,\n",
      "        3.0474, 1.8141, 2.4141, 2.4141, 1.6141, 1.6141, 1.4141, 1.6141, 2.2141,\n",
      "        2.8141, 2.4141, 2.2391, 2.6629, 2.7509, 3.1509, 2.7509, 2.7509, 2.7509,\n",
      "        2.7509, 2.5509, 2.5509, 1.9509, 1.9509, 2.6449, 1.8449, 2.2449, 1.9245,\n",
      "        2.4949, 2.4949, 2.0949, 2.0949, 2.0949, 2.8949, 2.1501, 3.1196, 2.1196,\n",
      "        2.3196, 2.4787, 2.4787, 2.0764, 1.8764, 2.6764, 2.4764, 2.6764, 1.8580,\n",
      "        2.0580, 2.6580, 2.9874, 2.9874, 2.2802, 2.3734, 1.9734, 1.5734, 1.5734,\n",
      "        3.1232, 2.9232, 3.1232, 2.9232, 2.4156, 2.4451, 1.8451, 1.8451, 1.8966,\n",
      "        3.0705, 3.0705, 3.0705, 3.0705, 2.4705, 2.2705, 2.2705, 2.8656, 2.8656,\n",
      "        2.6656, 2.2656, 2.2656, 2.7493, 2.4823, 3.0823, 3.0823, 2.0823, 2.3056,\n",
      "        2.7056, 2.7056, 2.7056, 2.7056, 1.9246, 2.5246, 2.5246, 2.9663, 2.6509,\n",
      "        2.4137, 2.4137, 3.0131, 3.0131, 2.5005, 1.9005, 1.9005, 2.1005, 2.1005,\n",
      "        2.1806, 2.1806, 2.3927, 2.3927, 2.3927, 2.3927, 2.7927, 2.8290, 2.8178,\n",
      "        2.8178, 2.5668, 2.5668, 2.5668, 2.1668, 2.1668, 2.7668, 1.7293, 1.7293,\n",
      "        2.1293, 2.1293, 1.8736, 1.9613, 1.3613, 1.5613, 2.6598, 2.6598, 2.5378,\n",
      "        2.0536, 2.0536, 2.0536, 2.3750, 2.4527, 2.5388, 2.1388, 1.6042, 1.7678,\n",
      "        1.7678, 2.9678, 2.4240, 2.6240, 2.6240, 2.6240, 2.6981, 2.2981, 1.7741,\n",
      "        1.5741, 1.3741, 1.3741, 1.3741, 1.1741, 1.7482, 1.9482, 2.3482, 2.8162,\n",
      "        2.8162, 1.9780, 1.5780, 2.0946, 2.3721, 2.3721, 2.3721, 2.3966, 2.3966,\n",
      "        2.1966, 2.9792, 2.9792, 1.6155, 2.1461, 2.0132, 2.3782, 1.9782, 1.9782,\n",
      "        1.7612, 1.7612, 3.1615, 2.7615, 2.0265, 2.2265, 2.6265, 2.6265, 3.0265,\n",
      "        2.8265, 2.8265, 2.6714, 2.6714, 2.6714, 2.6714, 2.0714, 1.8714, 1.8714,\n",
      "        3.2777, 3.1582, 2.1950, 2.1950, 2.6282, 2.9241, 1.7521, 1.7521, 2.3521,\n",
      "        1.9054, 1.9825, 1.9825, 1.9825, 3.0070, 1.3767, 1.3767, 1.7029, 2.8876,\n",
      "        2.8876, 3.2876, 1.9380, 2.2170, 2.2170, 2.2170, 2.4080, 3.0080, 3.0080,\n",
      "        2.4080, 3.2080, 2.7006, 2.7006, 2.9006, 2.9006, 2.5006, 2.7006, 2.3006,\n",
      "        1.9006, 1.9006, 1.9006, 2.3407, 2.5407, 2.5407, 2.8478, 3.0478, 2.5244,\n",
      "        1.9244, 1.9244, 1.9244, 1.9871, 2.2487, 2.0487, 2.0487, 2.4175, 2.2175,\n",
      "        2.2175, 2.2175, 2.2175, 2.2175, 2.7249, 2.7249, 2.3249, 2.3249, 2.3249,\n",
      "        2.3249, 2.1249, 3.1415, 3.0811, 3.0811, 1.4012, 1.4012, 2.0024, 2.8024,\n",
      "        3.0024, 3.1103, 2.0483, 2.5682, 2.5682, 2.5682, 2.6931, 3.8733, 3.4733,\n",
      "        3.4733, 3.0733, 2.8007, 3.4007, 2.8007, 2.8007, 3.0007, 2.8007, 2.8007,\n",
      "        2.6007, 2.4885, 2.2885, 2.0885, 1.4246, 1.4246, 1.8246, 1.8246, 1.8246,\n",
      "        2.4004, 3.2004, 3.1981, 2.7981, 2.7981, 2.7981, 2.5981, 2.2044, 2.2044,\n",
      "        2.2044, 2.6044, 2.6044, 2.2044, 2.2044, 2.8044, 2.8044, 2.8044, 2.0272,\n",
      "        2.0272, 2.6272, 2.6272, 2.2272, 2.0272, 2.0272, 2.6272, 1.8592, 2.3867,\n",
      "        2.4273, 2.4273, 3.0273, 3.0273, 3.0273, 3.0273, 3.0273, 2.6273, 2.6273,\n",
      "        2.4273, 3.4652, 2.2652, 2.9459, 2.7459, 2.7459, 1.2255, 1.2255, 1.2255,\n",
      "        1.2255, 1.2255, 1.0255, 1.6288, 1.6288, 2.2288, 2.2288, 2.2288, 2.6385,\n",
      "        3.0385, 2.7087, 3.1087, 2.5087, 2.5087, 1.9087, 2.1087, 2.1087, 2.2529,\n",
      "        2.2529, 1.6529, 2.0529, 2.0529, 2.0529, 2.0529, 2.0529, 2.4529, 2.2086,\n",
      "        2.4086, 2.4086, 2.1803, 2.1803, 2.1803, 2.3803, 1.9803, 2.1803, 1.7929,\n",
      "        2.0838, 2.0838, 2.5309, 1.7735, 2.6448, 2.2448, 3.3503, 3.2397, 3.2397,\n",
      "        3.2397, 3.2397, 3.2397, 2.8397, 2.8397, 2.8397, 2.8397, 2.4397, 1.9432,\n",
      "        2.1843, 1.7843, 2.1843, 2.2035, 1.8035, 2.8346, 2.4346, 2.8346, 2.4535,\n",
      "        2.4535, 1.8535, 1.8535, 1.8535, 2.2535, 2.2535, 2.0535, 2.0535, 2.0535,\n",
      "        2.2535, 2.2535, 2.0535, 2.0535, 1.9476, 1.9476, 2.5476, 2.5476, 2.5476,\n",
      "        2.1476, 2.5476, 2.5476, 2.1476, 2.3476, 2.2048, 2.5370, 2.3994, 2.3994,\n",
      "        2.3994, 2.6323, 2.6323, 2.6323, 2.2654, 2.2654, 2.2654, 2.2654, 2.2654,\n",
      "        2.2654, 2.2654, 2.2654, 2.2245, 2.2245, 2.2245, 1.4457, 2.2738, 2.4738,\n",
      "        2.4738, 2.4738, 2.2738, 1.4702, 2.3737, 2.3737, 2.3737, 2.3737, 2.3737,\n",
      "        2.3737, 2.3737, 2.5737, 2.5737, 1.9737, 2.7508, 1.7076, 1.7076, 3.4864,\n",
      "        3.4864, 3.4864, 2.2128, 2.3804, 2.3804, 2.1804, 2.3804, 2.3804, 1.9804,\n",
      "        2.3804, 2.5804, 2.4892, 2.4892, 2.6892, 2.6892, 2.6892, 2.4002, 2.2002,\n",
      "        3.4002, 3.6002, 3.6002, 1.2437, 3.4170, 2.2170, 2.8170, 2.8170, 1.4793,\n",
      "        2.0793, 2.0793, 1.4793, 2.2620, 1.6620, 1.6620, 2.0952, 2.4975, 2.4975,\n",
      "        2.9842, 2.3842, 3.1842, 3.0048, 3.0048, 2.4581, 2.4581, 2.0581, 2.0581,\n",
      "        1.9497, 1.9497, 2.3497, 1.9497, 2.5497, 2.9497, 3.0154, 2.8154, 2.6154,\n",
      "        2.6154, 2.1051, 2.6173, 2.0173, 2.6173, 2.6173, 3.2173],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "measurement_2 torch.Size([1000]) tensor([2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 1.5029, 0.1029, 0.1029, 0.1029,\n",
      "        0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029,\n",
      "        0.1029, 0.1029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 1.9029, 1.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 0.5029, 0.5029, 0.5029,\n",
      "        1.9029, 1.9029, 1.9029, 0.1029, 0.1029, 2.9029, 2.9029, 0.5029, 0.5029,\n",
      "        0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029,\n",
      "        0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029,\n",
      "        0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029,\n",
      "        0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 1.7029, 1.7029,\n",
      "        1.7029, 1.7029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 1.3029, 1.7029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.7029,\n",
      "        2.1029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 1.5029, 1.5029, 1.5029, 1.5029, 1.5029, 1.5029,\n",
      "        1.5029, 1.5029, 1.5029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 0.7029, 0.7029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 1.1029, 1.1029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 0.5029, 2.9029, 2.9029, 2.9029, 2.1029, 2.1029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 0.5029, 0.5029, 2.9029,\n",
      "        2.9029, 2.9029, 2.3029, 1.9029, 1.9029, 1.9029, 1.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 1.9029, 2.7029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        0.7029, 0.7029, 0.7029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 1.7029,\n",
      "        2.9029, 1.1029, 1.1029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 1.1029, 1.1029, 1.1029, 1.1029, 1.1029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 1.5029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.5029, 1.9029, 1.9029, 1.9029, 1.9029,\n",
      "        1.9029, 1.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 0.9029, 0.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.5029, 2.5029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 0.9029, 0.9029, 2.9029, 2.9029, 2.9029, 0.7029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 1.9029, 1.9029,\n",
      "        1.7029, 1.7029, 2.9029, 2.1029, 2.1029, 2.1029, 2.1029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 1.9029, 2.9029, 0.1029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 1.3029, 1.3029, 1.3029, 1.3029, 1.3029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 1.7029, 1.7029, 1.7029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.7029, 2.9029, 2.9029, 2.9029, 2.9029, 1.9029,\n",
      "        1.9029, 1.9029, 1.9029, 2.9029, 0.7029, 2.9029, 2.9029, 2.9029, 2.1029,\n",
      "        2.1029, 2.9029, 2.1029, 2.1029, 2.1029, 2.1029, 2.1029, 0.1029, 0.1029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 1.1029, 1.1029, 1.1029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 0.5029, 0.5029, 0.5029, 0.5029,\n",
      "        0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 2.9029, 2.9029, 2.9029, 1.3029,\n",
      "        1.3029, 1.3029, 1.3029, 1.3029, 1.3029, 1.3029, 1.3029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 1.9029, 1.9029, 1.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 1.5029, 1.5029, 1.5029, 1.5029, 2.1029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 0.5029, 0.5029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 1.1029, 0.9029, 0.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.1029, 2.1029,\n",
      "        2.1029, 2.1029, 2.1029, 2.1029, 2.1029, 0.1029, 0.1029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        0.9029, 0.9029, 0.9029, 0.9029, 0.9029, 0.9029, 0.9029, 0.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 1.1029, 1.1029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 0.1029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 1.5029, 1.5029, 1.5029, 1.5029, 1.5029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.1029, 2.1029, 2.1029, 2.1029, 2.1029,\n",
      "        2.1029, 0.5029, 0.5029, 0.5029, 0.5029, 0.5029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 0.3029, 0.3029, 0.3029, 0.3029, 0.3029,\n",
      "        0.3029, 0.3029, 0.9029, 0.9029, 0.9029, 0.9029, 0.9029, 0.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 0.9029,\n",
      "        0.9029, 0.9029, 0.9029, 0.9029, 0.9029, 0.9029, 0.9029, 0.9029, 0.9029,\n",
      "        0.9029, 0.9029, 0.9029, 0.9029, 0.9029, 0.9029, 2.5029, 2.9029, 2.9029,\n",
      "        2.9029, 2.1029, 2.1029, 1.5029, 2.9029, 2.9029, 2.9029, 1.3029, 1.3029,\n",
      "        1.3029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 0.1029, 0.1029, 2.9029, 0.5029, 0.5029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 1.9029, 1.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 1.9029, 1.9029, 1.9029, 1.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 1.7029, 1.7029, 1.7029, 1.7029, 1.7029, 1.7029, 0.9029,\n",
      "        0.9029, 0.9029, 2.3029, 2.1029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 1.1029, 1.1029, 1.1029, 1.1029, 1.1029, 1.1029, 1.1029, 1.1029,\n",
      "        1.1029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 1.9029, 1.9029, 2.9029, 2.9029, 2.5029, 2.5029,\n",
      "        2.5029, 1.3029, 1.3029, 1.3029, 1.3029, 1.3029, 1.3029, 1.3029, 1.3029,\n",
      "        1.3029, 1.3029, 1.3029, 1.3029, 1.3029, 1.3029, 1.3029, 1.3029, 2.1029,\n",
      "        2.1029, 2.1029, 2.1029, 2.1029, 2.1029, 2.1029, 2.1029, 2.1029, 2.9029,\n",
      "        0.3029, 0.3029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.1029, 2.1029,\n",
      "        2.1029, 0.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029, 2.9029,\n",
      "        2.9029], grad_fn=<ReshapeAliasBackward0>)\n",
      "measurement_1 torch.Size([1000]) tensor([3.2340, 3.6340, 3.6340, 3.2340, 3.4340, 2.1340, 3.4340, 3.4340, 3.4340,\n",
      "        3.4340, 3.4340, 3.4340, 3.4340, 3.4340, 3.4340, 3.4340, 3.4340, 3.4340,\n",
      "        3.4340, 3.4340, 3.7340, 3.7340, 3.7340, 3.5340, 3.5340, 3.5340, 2.9340,\n",
      "        2.8340, 3.2340, 3.2340, 2.7340, 2.7340, 3.2340, 3.4340, 2.5340, 2.5340,\n",
      "        2.9340, 3.1340, 3.8340, 3.2340, 2.8340, 3.0340, 3.5340, 3.5340, 3.5340,\n",
      "        2.6340, 2.6340, 2.6340, 3.3340, 3.3340, 3.2340, 3.2340, 3.6340, 3.6340,\n",
      "        3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340,\n",
      "        3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340,\n",
      "        3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340,\n",
      "        3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 2.6340, 2.6340,\n",
      "        2.6340, 2.6340, 3.6340, 2.4340, 2.4340, 2.4340, 2.4340, 3.0340, 3.0340,\n",
      "        2.4340, 2.4340, 3.0340, 3.8340, 3.6340, 3.1340, 2.5340, 3.1340, 3.1340,\n",
      "        3.1340, 3.1340, 3.1340, 3.3340, 3.0340, 3.0340, 3.8340, 3.8340, 3.6340,\n",
      "        3.4340, 3.4340, 3.3340, 3.3340, 2.9340, 2.9340, 3.7340, 3.9340, 3.9340,\n",
      "        3.7340, 3.7340, 3.7340, 3.7340, 3.5340, 3.1340, 3.3340, 3.1340, 3.5340,\n",
      "        3.5340, 3.1340, 3.1340, 2.7340, 3.1340, 2.9340, 2.9340, 3.7340, 3.7340,\n",
      "        3.7340, 3.7340, 3.9340, 3.3340, 3.3340, 3.3340, 3.3340, 3.3340, 3.3340,\n",
      "        3.3340, 3.3340, 3.3340, 3.8340, 4.0340, 4.0340, 4.0340, 3.8340, 3.8340,\n",
      "        3.8340, 3.8340, 3.4340, 4.0340, 2.4340, 2.4340, 3.6340, 3.0340, 3.3340,\n",
      "        3.5340, 3.5340, 3.5340, 3.1340, 3.0340, 3.0340, 3.2340, 3.2340, 4.1340,\n",
      "        3.5340, 3.5340, 2.9340, 2.9340, 2.5340, 2.5340, 2.5340, 2.5340, 2.1340,\n",
      "        2.1340, 2.6340, 2.8340, 2.9340, 2.9340, 2.9340, 2.9340, 2.7340, 2.7340,\n",
      "        2.5340, 2.6340, 3.4340, 3.8340, 3.8340, 2.5340, 2.5340, 3.5340, 3.7340,\n",
      "        3.7340, 3.0340, 3.0340, 3.0340, 3.6340, 3.8340, 3.3340, 3.3340, 3.5340,\n",
      "        3.7340, 3.2340, 3.6340, 3.0340, 3.0340, 3.0340, 2.9340, 2.9340, 3.3340,\n",
      "        2.9340, 3.6340, 3.6340, 3.6340, 3.6340, 3.2340, 2.5340, 2.5340, 3.5340,\n",
      "        3.1340, 3.3340, 3.2340, 2.3340, 2.3340, 2.3340, 2.3340, 3.0340, 3.0340,\n",
      "        2.6340, 2.6340, 2.4340, 2.4340, 2.4340, 3.0340, 3.4340, 3.4340, 3.4340,\n",
      "        3.4340, 3.0340, 3.4340, 3.4340, 2.8340, 2.8340, 2.4340, 3.4340, 3.2340,\n",
      "        3.4340, 3.2340, 3.1340, 3.4340, 3.1340, 3.2340, 3.4340, 3.4340, 3.4340,\n",
      "        3.2340, 3.2340, 3.6340, 3.6340, 2.8340, 2.8340, 2.9340, 3.5340, 3.3340,\n",
      "        3.2340, 3.1340, 3.1340, 3.5340, 3.5340, 3.5340, 3.1340, 3.5340, 3.2340,\n",
      "        3.0340, 3.0340, 3.2340, 3.2340, 3.0340, 3.2340, 3.2340, 3.2340, 3.4340,\n",
      "        3.0340, 3.0340, 3.0340, 3.3340, 3.5340, 3.7340, 3.4340, 3.4340, 3.6340,\n",
      "        2.9340, 2.6340, 2.6340, 2.5340, 3.1340, 2.7340, 2.7340, 3.4340, 4.0340,\n",
      "        3.4340, 3.8340, 2.9340, 2.9340, 2.9340, 2.9340, 2.9340, 3.2340, 3.4340,\n",
      "        3.2340, 3.2340, 2.8340, 2.8340, 2.8340, 2.8340, 2.8340, 2.8340, 3.7340,\n",
      "        3.5340, 3.5340, 3.5340, 3.5340, 3.5340, 3.1340, 3.1340, 2.2340, 3.4340,\n",
      "        3.4340, 3.4340, 3.6340, 2.8340, 3.7340, 2.5340, 2.5340, 2.5340, 2.5340,\n",
      "        2.5340, 2.5340, 3.4340, 3.4340, 3.4340, 3.4340, 3.1340, 3.3340, 3.3340,\n",
      "        3.3340, 3.3340, 3.5340, 3.3340, 3.3340, 3.2340, 3.4340, 2.8340, 2.8340,\n",
      "        3.7340, 3.7340, 3.6340, 3.6340, 2.6340, 3.4340, 3.4340, 2.6340, 3.6340,\n",
      "        3.2340, 3.2340, 2.8340, 3.0340, 3.0340, 3.4340, 3.6340, 3.3340, 2.9340,\n",
      "        2.9340, 3.1340, 3.1340, 3.1340, 3.1340, 3.1340, 3.3340, 3.2340, 3.0340,\n",
      "        2.8340, 2.6340, 3.3340, 3.5340, 3.5340, 2.4340, 2.6340, 3.0340, 2.8340,\n",
      "        3.6340, 3.6340, 3.5340, 3.2340, 3.2340, 3.2340, 3.2340, 3.5340, 3.5340,\n",
      "        3.3340, 3.3340, 3.9340, 3.3340, 3.3340, 3.3340, 3.3340, 3.4340, 3.4340,\n",
      "        3.1340, 3.3340, 3.3340, 3.1340, 3.4340, 3.4340, 3.4340, 3.4340, 3.8340,\n",
      "        3.6340, 2.5340, 2.9340, 2.7340, 2.7340, 3.1340, 2.7340, 2.7340, 3.1340,\n",
      "        2.9340, 3.2340, 3.2340, 2.5340, 2.7340, 3.2340, 3.3340, 3.3340, 3.3340,\n",
      "        2.7340, 3.1340, 2.7340, 4.0340, 4.0340, 2.7340, 3.3340, 2.6340, 3.1340,\n",
      "        3.4340, 2.9340, 3.3340, 3.3340, 2.6340, 2.6340, 2.6340, 2.6340, 2.6340,\n",
      "        2.7340, 2.7340, 3.3340, 3.5340, 3.4340, 3.4340, 3.2340, 3.2340, 3.6340,\n",
      "        3.4340, 3.4340, 3.8340, 3.8340, 3.8340, 3.8340, 3.8340, 3.8340, 3.8340,\n",
      "        3.8340, 3.6340, 3.6340, 3.0340, 3.4340, 3.6340, 3.6340, 3.7340, 3.4340,\n",
      "        3.4340, 3.4340, 3.4340, 3.1340, 4.2340, 2.6340, 2.6340, 3.0340, 2.4340,\n",
      "        2.4340, 3.5340, 2.3340, 2.3340, 2.3340, 2.3340, 2.3340, 2.9340, 2.9340,\n",
      "        2.8340, 2.8340, 2.8340, 3.4340, 3.0340, 2.6340, 2.4340, 3.4340, 3.4340,\n",
      "        3.8340, 2.9340, 3.2340, 3.2340, 3.2340, 2.6340, 2.6340, 2.6340, 3.1340,\n",
      "        3.3340, 3.3340, 3.5340, 3.5340, 3.6340, 3.6340, 3.8340, 3.8340, 3.8340,\n",
      "        3.8340, 3.8340, 3.0340, 3.0340, 3.0340, 3.3340, 3.3340, 3.3340, 3.3340,\n",
      "        3.3340, 3.3340, 3.3340, 3.3340, 3.3340, 3.7340, 3.3340, 3.3340, 2.5340,\n",
      "        2.5340, 2.5340, 2.5340, 2.5340, 2.5340, 2.5340, 2.5340, 3.1340, 3.9340,\n",
      "        3.0340, 2.8340, 2.8340, 2.8340, 3.3340, 3.3340, 3.3340, 2.9340, 3.0340,\n",
      "        3.0340, 3.0340, 2.9340, 2.9340, 2.9340, 2.9340, 3.1340, 2.8340, 3.2340,\n",
      "        3.2340, 3.2340, 3.2340, 3.2340, 3.3340, 3.3340, 3.1340, 3.1340, 3.3340,\n",
      "        3.3340, 2.7340, 3.3340, 3.4340, 3.4340, 3.2340, 3.0340, 2.8340, 2.8340,\n",
      "        3.2340, 3.2340, 3.6340, 3.8340, 3.4340, 2.1340, 2.7340, 2.7340, 4.1340,\n",
      "        4.1340, 4.1340, 2.8340, 4.0340, 3.4340, 3.4340, 3.4340, 3.0340, 3.0340,\n",
      "        3.0340, 3.0340, 3.0340, 3.0340, 3.0340, 2.8340, 2.8340, 3.8340, 3.4340,\n",
      "        3.4340, 3.4340, 3.4340, 3.2340, 3.2340, 3.6340, 3.8340, 3.8340, 3.6340,\n",
      "        3.5340, 3.5340, 3.5340, 3.5340, 3.5340, 3.5340, 3.5340, 3.5340, 2.3340,\n",
      "        2.5340, 3.1340, 3.1340, 3.1340, 2.3340, 2.3340, 3.7340, 3.7340, 3.9340,\n",
      "        3.5340, 3.5340, 3.5340, 3.1340, 3.3340, 3.3340, 3.3340, 2.9340, 2.9340,\n",
      "        3.3340, 3.3340, 3.1340, 3.1340, 3.1340, 2.9340, 2.9340, 3.7340, 3.7340,\n",
      "        3.5340, 2.7340, 3.3340, 3.9340, 2.4340, 2.9340, 3.3340, 3.4340, 3.4340,\n",
      "        3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.0340, 3.0340, 2.8340, 3.6340,\n",
      "        3.8340, 2.1340, 2.1340, 2.1340, 2.1340, 2.1340, 3.8340, 4.0340, 4.0340,\n",
      "        2.5340, 2.7340, 2.7340, 2.7340, 2.7340, 2.3340, 3.5340, 3.5340, 3.3340,\n",
      "        3.3340, 3.3340, 3.8340, 4.0340, 2.2340, 2.2340, 2.2340, 2.2340, 2.2340,\n",
      "        2.2340, 3.3340, 3.3340, 3.3340, 3.3340, 3.3340, 3.9340, 3.9340, 3.5340,\n",
      "        3.5340, 2.9340, 2.7340, 2.7340, 3.8340, 3.8340, 3.8340, 3.8340, 3.8340,\n",
      "        3.8340, 3.8340, 3.8340, 3.8340, 3.8340, 3.8340, 3.8340, 3.8340, 3.0340,\n",
      "        3.0340, 2.8340, 2.6340, 3.0340, 3.0340, 3.0340, 3.2340, 3.2340, 3.5340,\n",
      "        3.1340, 3.1340, 2.8340, 2.8340, 2.8340, 3.2340, 2.8340, 3.6340, 3.2340,\n",
      "        3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340,\n",
      "        3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 2.9340, 2.8340, 2.8340,\n",
      "        3.7340, 2.2340, 2.2340, 2.3340, 3.3340, 3.1340, 3.1340, 3.0340, 3.0340,\n",
      "        3.0340, 3.6340, 3.7340, 3.7340, 3.7340, 3.7340, 3.7340, 3.7340, 3.7340,\n",
      "        3.5340, 3.1340, 2.9340, 3.0340, 3.0340, 3.3340, 2.1340, 2.1340, 3.7340,\n",
      "        3.3340, 3.5340, 2.9340, 2.9340, 3.4340, 3.4340, 3.8340, 3.0340, 3.4340,\n",
      "        3.0340, 3.0340, 3.0340, 2.8340, 2.8340, 3.4340, 3.4340, 3.4340, 3.2340,\n",
      "        2.8340, 2.8340, 2.8340, 3.6340, 3.6340, 2.7340, 2.7340, 2.9340, 2.9340,\n",
      "        3.7340, 2.7340, 3.5340, 3.5340, 3.3340, 2.9340, 2.7340, 3.4340, 3.2340,\n",
      "        3.2340, 3.2340, 3.6340, 3.6340, 3.6340, 3.6340, 3.0340, 3.0340, 3.0340,\n",
      "        3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.0340,\n",
      "        3.0340, 3.0340, 2.4340, 2.4340, 2.4340, 2.4340, 2.4340, 2.4340, 3.7340,\n",
      "        3.7340, 3.7340, 2.9340, 3.3340, 3.3340, 3.7340, 3.5340, 3.5340, 2.9340,\n",
      "        2.7340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340, 3.6340,\n",
      "        3.6340, 3.5340, 3.5340, 3.5340, 2.9340, 3.5340, 3.5340, 3.5340, 2.9340,\n",
      "        2.9340, 3.1340, 3.4340, 2.5340, 2.5340, 2.6340, 2.6340, 3.7340, 3.7340,\n",
      "        3.7340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340,\n",
      "        3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 3.2340, 2.6340,\n",
      "        2.6340, 2.6340, 2.6340, 2.6340, 2.6340, 2.6340, 2.6340, 2.6340, 3.5340,\n",
      "        2.4340, 2.4340, 3.4340, 3.6340, 3.6340, 3.0340, 3.0340, 3.0340, 3.0340,\n",
      "        2.8340, 3.7340, 3.7340, 3.7340, 3.7340, 3.7340, 3.8340, 3.4340, 4.0340,\n",
      "        4.2340, 4.2340, 2.6340, 4.2340, 3.8340, 3.2340, 3.2340, 2.3340, 2.3340,\n",
      "        2.3340, 3.3340, 2.5340, 2.5340, 2.5340, 2.9340, 2.7340, 2.9340, 2.9340,\n",
      "        2.7340, 3.1340, 3.1340, 3.2340, 3.4340, 3.6340, 3.1340, 3.1340, 2.9340,\n",
      "        2.9340, 3.1340, 3.1340, 3.3340, 3.3340, 3.5340, 3.3340, 3.5340, 3.5340,\n",
      "        3.3340, 3.9340, 3.1340, 3.1340, 2.6340, 2.6340, 3.6340, 3.8340, 3.8340,\n",
      "        4.0340], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from pyro.infer.importance import Importance\n",
    "import pyro.infer.mcmc as pyromcmc\n",
    "from pathlib import Path\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "kernel = pyromcmc.NPDHMC(\n",
    "    scale,\n",
    "    step_size=0.1,\n",
    "    num_steps=50,\n",
    "    adapt_step_size=False,\n",
    ")\n",
    "count = 1000\n",
    "mcmc = pyromcmc.MCMC(kernel, num_samples=count, warmup_steps=count // 10)\n",
    "mcmc.run()\n",
    "samples = mcmc.get_samples()\n",
    "for key, value in samples.items():\n",
    "    print(key, value.shape, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c5e242f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3915, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(samples[\"weight\"].mean(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8cb51e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 1100/1100 [00:58, 18.89it/s, step size=1.00e-01, acc. prob=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'measurement_1': torch.Size([]), 'weight': torch.Size([])}\n",
      "tensor(2.1717, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kernel = pyromcmc.HMC(\n",
    "    scale,\n",
    "    step_size=0.1,\n",
    "    num_steps=50,\n",
    "    adapt_step_size=False,\n",
    ")\n",
    "count = 1000\n",
    "mcmc = pyromcmc.MCMC(kernel, num_samples=count, warmup_steps=count // 10)\n",
    "mcmc.run()\n",
    "samples = mcmc.get_samples()\n",
    "print(samples[\"weight\"].mean(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
