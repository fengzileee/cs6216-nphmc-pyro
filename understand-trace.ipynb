{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90685b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Callable, Generic, TypeVar\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Normal, Uniform\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d5113",
   "metadata": {},
   "source": [
    "# Custom PPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfffca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class ProbCtx:\n",
    "    \"\"\"Probabilistic context: keeps track of a probabilistic execution (samples, weight, etc.)\"\"\"\n",
    "\n",
    "    def __init__(self, trace: torch.Tensor) -> None:\n",
    "        self.idx = 0\n",
    "        \"\"\"Index/address of the next sample variable\"\"\"\n",
    "        self.samples = trace.clone().detach()\n",
    "        \"\"\"Sampled values so far in the trace\"\"\"\n",
    "        self.samples.requires_grad_(True)\n",
    "        \"\"\"The given sample vector\"\"\"\n",
    "        self.is_cont: torch.Tensor = torch.ones(self.samples.shape, dtype=torch.bool)\n",
    "        \"\"\"Whether the sampled value is continuous.\n",
    "\n",
    "        A sample is discontinuous if a branch in the program depends on it.\"\"\"\n",
    "        self.log_weight = torch.tensor(0.0, requires_grad=True)\n",
    "        \"\"\"Logarithm of the weight.\n",
    "\n",
    "        The weight by the pdf for sample()s as well as score()s\"\"\"\n",
    "        self.log_score = torch.tensor(0.0, requires_grad=True)\n",
    "        \"\"\"Logarithm of the score.\n",
    "\n",
    "        The score is only multiplied for score()\"\"\"\n",
    "\n",
    "        \"\"\" The log weight of the trace given \"\"\"\n",
    "        self.sample_logps = torch.zeros(trace.size())\n",
    "        \"\"\"Records the log probability of each sample.\"\"\"\n",
    "\n",
    "    def constrain(\n",
    "        self,\n",
    "        sample,\n",
    "        geq: float = None,\n",
    "        lt: float = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Constrains the sample to be >= geq and < lt.\n",
    "\n",
    "        This is necessary for random variables whose support isn't all reals.\n",
    "\n",
    "        Args:\n",
    "            sample: the sample to be constrained\n",
    "            geq (float, optional): The lower bound. Defaults to None.\n",
    "            lt (float, optional): The upper bound. Defaults to None.\n",
    "        \"\"\"\n",
    "        if lt is not None:\n",
    "            if sample >= lt:\n",
    "                self.score_log(torch.tensor(-math.inf))\n",
    "        if geq is not None:\n",
    "            if sample <= geq:\n",
    "                self.score_log(torch.tensor(-math.inf))\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        dist: torch.distributions.Distribution,\n",
    "        is_cont: bool,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Samples from the given distribution.\n",
    "\n",
    "        If the distribution has support not on all reals, this needs to be followed by suitable constrain() calls.\n",
    "\n",
    "        Args:\n",
    "            dist (torch.distributions.Distribution): the distribution to sample from\n",
    "            is_cont (bool): whether or not the weight function is continuous in this variable\n",
    "\n",
    "        Returns:\n",
    "            the sample\n",
    "        \"\"\"\n",
    "        samples = self.sample_n(1, dist, is_cont)\n",
    "        return samples[0]\n",
    "\n",
    "    def sample_n(\n",
    "        self,\n",
    "        n: int,\n",
    "        dist: torch.distributions.Distribution,\n",
    "        is_cont: bool,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Samples n times from the given distribution.\n",
    "\n",
    "        Args:\n",
    "            n (int): the number of samples\n",
    "            dist (torch.distributions.Distribution): the distribution to sample from\n",
    "            is_cont (bool): whether or not the weight function is continuous in this variable\n",
    "\n",
    "        Returns:\n",
    "            the samples\n",
    "        \"\"\"\n",
    "        needed = self.idx + n - len(self.samples)\n",
    "        if needed > 0:\n",
    "            values = dist.sample((needed,))\n",
    "            values.requires_grad_(True)\n",
    "            self.samples = torch.cat((self.samples, values))\n",
    "            self.sample_logps = torch.cat([self.sample_logps, torch.zeros((needed,))])\n",
    "            self.is_cont = torch.cat(\n",
    "                (self.is_cont, torch.ones(needed, dtype=torch.bool))\n",
    "            )\n",
    "        for i in range(self.idx, self.idx + n):\n",
    "            if math.isnan(self.samples[i]):\n",
    "                self.samples[i] = dist.sample(())\n",
    "        values = self.samples[self.idx : self.idx + n]\n",
    "        self.sample_logps[self.idx : self.idx + n] = dist.log_prob(values)\n",
    "        self.is_cont[self.idx : self.idx + n] = torch.tensor(is_cont).repeat(n)\n",
    "        self.log_weight = self.log_weight + torch.sum(dist.log_prob(values))\n",
    "        self.idx += n\n",
    "        return values\n",
    "\n",
    "    def score(self, weight: torch.Tensor) -> None:\n",
    "        \"\"\"Multiplies the current trace by the given weight.\n",
    "\n",
    "        Args:\n",
    "            weight (torch.Tensor): the weight.\n",
    "        \"\"\"\n",
    "        assert torch.is_tensor(weight), \"weight is not a tensor\"\n",
    "        self.score_log(torch.log(weight))\n",
    "\n",
    "    def score_log(self, log_weight: torch.Tensor) -> None:\n",
    "        assert torch.is_tensor(log_weight), \"weight is not a tensor\"\n",
    "        self.log_weight = self.log_weight + log_weight\n",
    "        self.log_score = self.log_score + log_weight\n",
    "\n",
    "    def observe(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        dist: torch.distributions.Distribution,\n",
    "    ) -> None:\n",
    "        self.score_log(dist.log_prob(obs))\n",
    "\n",
    "\n",
    "class ProbRun(Generic[T]):\n",
    "    \"\"\"Result of a probabilistic run\"\"\"\n",
    "\n",
    "    def __init__(self, ctx: ProbCtx, value: T) -> None:\n",
    "        \"\"\"Creates a probabilistic run result.\n",
    "\n",
    "        Undocumented fields are the same as for ProbCtx\n",
    "\n",
    "        Args:\n",
    "            ctx (ProbCtx): the probabilistic context used for the program.\n",
    "            value (T): the return value of the probabilistic program.\n",
    "        \"\"\"\n",
    "        self._gradU: torch.Tensor = None\n",
    "        \"\"\"Caches the gradient.\"\"\"\n",
    "        self.log_weight = ctx.log_weight\n",
    "        self.log_score = ctx.log_score\n",
    "        self.samples = ctx.samples\n",
    "        self.len = ctx.idx\n",
    "        \"\"\"Number of sample statements encountered, i.e. length of the trace.\"\"\"\n",
    "        self.is_cont = ctx.is_cont\n",
    "        self.value = value\n",
    "        \"\"\"Returned value of the probabilistic program.\"\"\"\n",
    "        self.sample_logps = ctx.sample_logps\n",
    "\n",
    "    def gradU(self) -> torch.Tensor:\n",
    "        if self._gradU is not None:\n",
    "            return self._gradU\n",
    "        U = -self.log_weight\n",
    "        (self._gradU,) = torch.autograd.grad(U, self.samples, allow_unused=True)\n",
    "        if self._gradU is None:\n",
    "            self._gradU = torch.zeros(self.samples.shape)\n",
    "        return self._gradU\n",
    "\n",
    "    def used_samples(self) -> torch.Tensor:\n",
    "        return self.samples[: self.len]\n",
    "\n",
    "\n",
    "def run_prob_prog(program: Callable[[ProbCtx], T], trace: torch.Tensor) -> ProbRun[T]:\n",
    "    \"\"\"Runs the given probabilistic program on the given trace.\n",
    "\n",
    "    Args:\n",
    "        program (Callable[[ProbCtx], T]): the probabilistic program.\n",
    "        trace (torch.Tensor): the trace to replay.\n",
    "\n",
    "    Returns:\n",
    "        ProbRun: the result of the probabilistic run.\n",
    "    \"\"\"\n",
    "    tensor_trace = trace\n",
    "    while True:\n",
    "        ctx = ProbCtx(tensor_trace)\n",
    "        ret = None\n",
    "        try:\n",
    "            ret = program(ctx)\n",
    "        except Exception as e:\n",
    "            if ctx.log_score.item() > -math.inf or ctx.log_weight.item() > -math.inf:\n",
    "                print(\"Exception in code with nonzero weight!\")\n",
    "                raise e\n",
    "            else:\n",
    "                print(\"Info: exception in branch with zero weight\")\n",
    "        if ctx.idx > len(tensor_trace):\n",
    "            tensor_trace = ctx.samples\n",
    "            continue\n",
    "        return ProbRun(ctx, ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73e2217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_model(ctx: ProbCtx) -> float:\n",
    "    \"\"\"Random walk model.\n",
    "\n",
    "    Mak et al. (2020): Densities of almost-surely terminating probabilistic programs are differentiable almost everywhere.\n",
    "    \"\"\"\n",
    "    distance = torch.tensor(0.0, requires_grad=True)\n",
    "    start = ctx.sample(Uniform(0, 3), is_cont=False)\n",
    "    position = start\n",
    "    while position > 0 and distance < 10:\n",
    "        step = ctx.sample(Uniform(-1, 1), is_cont=False)\n",
    "        distance = distance + torch.abs(step)\n",
    "        position = position + step\n",
    "    ctx.observe(distance, Normal(1.1, 0.1))\n",
    "    return start.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d74abdd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'constrain',\n",
       " 'idx',\n",
       " 'is_cont',\n",
       " 'log_score',\n",
       " 'log_weight',\n",
       " 'observe',\n",
       " 'sample',\n",
       " 'sample_logps',\n",
       " 'sample_n',\n",
       " 'samples',\n",
       " 'score',\n",
       " 'score_log']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace = torch.tensor([])\n",
    "ctx = ProbCtx(trace)\n",
    "walk_model(ctx)\n",
    "dir(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1754bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.6704,  0.7230, -0.3760, -0.7057, -0.3816,  0.5948,  0.6755, -0.2594,\n",
       "         0.0581, -0.6915,  0.7120, -0.2331, -0.4883,  0.4422, -0.8988, -0.2762,\n",
       "         0.2867, -0.7014, -0.7489], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc5a485",
   "metadata": {},
   "source": [
    "# Pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "586da93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "\n",
    "def pyro_walk_model():\n",
    "    start = pyro.sample(\"start\", pyro.distributions.Uniform(0, 3))\n",
    "    t = 0\n",
    "    position = start\n",
    "    distance = torch.tensor(0.0)\n",
    "    while position > 0 and position < 10:\n",
    "        step = pyro.sample(f\"step_{t}\", pyro.distributions.Uniform(-1, 1))\n",
    "        distance = distance + torch.abs(step)\n",
    "        position = position + step\n",
    "        t = t + 1\n",
    "    pyro.sample(\"obs\", pyro.distributions.Normal(1.1, 0.1), obs=distance)\n",
    "    return start.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "345718bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('_INPUT',\n",
       "              {'name': '_INPUT', 'type': 'args', 'args': (), 'kwargs': {}}),\n",
       "             ('start',\n",
       "              {'type': 'sample',\n",
       "               'name': 'start',\n",
       "               'fn': Uniform(low: 0.0, high: 3.0),\n",
       "               'is_observed': False,\n",
       "               'args': (),\n",
       "               'kwargs': {},\n",
       "               'value': tensor(1.0567),\n",
       "               'infer': {},\n",
       "               'scale': 1.0,\n",
       "               'mask': None,\n",
       "               'cond_indep_stack': (),\n",
       "               'done': True,\n",
       "               'stop': False,\n",
       "               'continuation': None}),\n",
       "             ('step_0',\n",
       "              {'type': 'sample',\n",
       "               'name': 'step_0',\n",
       "               'fn': Uniform(low: -1.0, high: 1.0),\n",
       "               'is_observed': False,\n",
       "               'args': (),\n",
       "               'kwargs': {},\n",
       "               'value': tensor(-0.0357),\n",
       "               'infer': {},\n",
       "               'scale': 1.0,\n",
       "               'mask': None,\n",
       "               'cond_indep_stack': (),\n",
       "               'done': True,\n",
       "               'stop': False,\n",
       "               'continuation': None}),\n",
       "             ('step_1',\n",
       "              {'type': 'sample',\n",
       "               'name': 'step_1',\n",
       "               'fn': Uniform(low: -1.0, high: 1.0),\n",
       "               'is_observed': False,\n",
       "               'args': (),\n",
       "               'kwargs': {},\n",
       "               'value': tensor(-0.7780),\n",
       "               'infer': {},\n",
       "               'scale': 1.0,\n",
       "               'mask': None,\n",
       "               'cond_indep_stack': (),\n",
       "               'done': True,\n",
       "               'stop': False,\n",
       "               'continuation': None}),\n",
       "             ('step_2',\n",
       "              {'type': 'sample',\n",
       "               'name': 'step_2',\n",
       "               'fn': Uniform(low: -1.0, high: 1.0),\n",
       "               'is_observed': False,\n",
       "               'args': (),\n",
       "               'kwargs': {},\n",
       "               'value': tensor(0.9786),\n",
       "               'infer': {},\n",
       "               'scale': 1.0,\n",
       "               'mask': None,\n",
       "               'cond_indep_stack': (),\n",
       "               'done': True,\n",
       "               'stop': False,\n",
       "               'continuation': None}),\n",
       "             ('step_3',\n",
       "              {'type': 'sample',\n",
       "               'name': 'step_3',\n",
       "               'fn': Uniform(low: -1.0, high: 1.0),\n",
       "               'is_observed': False,\n",
       "               'args': (),\n",
       "               'kwargs': {},\n",
       "               'value': tensor(-0.2054),\n",
       "               'infer': {},\n",
       "               'scale': 1.0,\n",
       "               'mask': None,\n",
       "               'cond_indep_stack': (),\n",
       "               'done': True,\n",
       "               'stop': False,\n",
       "               'continuation': None}),\n",
       "             ('step_4',\n",
       "              {'type': 'sample',\n",
       "               'name': 'step_4',\n",
       "               'fn': Uniform(low: -1.0, high: 1.0),\n",
       "               'is_observed': False,\n",
       "               'args': (),\n",
       "               'kwargs': {},\n",
       "               'value': tensor(-0.9605),\n",
       "               'infer': {},\n",
       "               'scale': 1.0,\n",
       "               'mask': None,\n",
       "               'cond_indep_stack': (),\n",
       "               'done': True,\n",
       "               'stop': False,\n",
       "               'continuation': None}),\n",
       "             ('step_5',\n",
       "              {'type': 'sample',\n",
       "               'name': 'step_5',\n",
       "               'fn': Uniform(low: -1.0, high: 1.0),\n",
       "               'is_observed': False,\n",
       "               'args': (),\n",
       "               'kwargs': {},\n",
       "               'value': tensor(-0.6760),\n",
       "               'infer': {},\n",
       "               'scale': 1.0,\n",
       "               'mask': None,\n",
       "               'cond_indep_stack': (),\n",
       "               'done': True,\n",
       "               'stop': False,\n",
       "               'continuation': None}),\n",
       "             ('obs',\n",
       "              {'type': 'sample',\n",
       "               'name': 'obs',\n",
       "               'fn': Normal(loc: 1.100000023841858, scale: 0.10000000149011612),\n",
       "               'is_observed': True,\n",
       "               'args': (),\n",
       "               'kwargs': {},\n",
       "               'value': tensor(3.6343),\n",
       "               'infer': {},\n",
       "               'scale': 1.0,\n",
       "               'mask': None,\n",
       "               'cond_indep_stack': (),\n",
       "               'done': True,\n",
       "               'stop': False,\n",
       "               'continuation': None}),\n",
       "             ('_RETURN',\n",
       "              {'name': '_RETURN',\n",
       "               'type': 'return',\n",
       "               'value': 1.0567030906677246})])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro_trace = pyro.poutine.trace(pyro_walk_model).get_trace()\n",
    "pyro_trace.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d42fcc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6931)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro_trace.nodes['step_3']['fn'].log_prob(pyro_trace.nodes['step_3']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9072a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro_trace.nodes['step_0']['value'] = torch.tensor(-0.99)\n",
    "pyro_trace.nodes['step_1']['value'] = torch.tensor(-0.99)\n",
    "pyro_trace = pyro.poutine.trace(pyro.poutine.replay(pyro_walk_model, trace=pyro_trace)).get_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7108f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('_INPUT',\n",
       "              {'name': '_INPUT', 'type': 'args', 'args': (), 'kwargs': {}}),\n",
       "             ('start',\n",
       "              {'type': 'sample',\n",
       "               'name': 'start',\n",
       "               'fn': Uniform(low: 0.0, high: 3.0),\n",
       "               'is_observed': False,\n",
       "               'args': (),\n",
       "               'kwargs': {},\n",
       "               'value': tensor(1.0567),\n",
       "               'infer': {},\n",
       "               'scale': 1.0,\n",
       "               'mask': None,\n",
       "               'cond_indep_stack': (),\n",
       "               'done': True,\n",
       "               'stop': False,\n",
       "               'continuation': None}),\n",
       "             ('step_0',\n",
       "              {'type': 'sample',\n",
       "               'name': 'step_0',\n",
       "               'fn': Uniform(low: -1.0, high: 1.0),\n",
       "               'is_observed': False,\n",
       "               'args': (),\n",
       "               'kwargs': {},\n",
       "               'value': tensor(-0.9900),\n",
       "               'infer': {},\n",
       "               'scale': 1.0,\n",
       "               'mask': None,\n",
       "               'cond_indep_stack': (),\n",
       "               'done': True,\n",
       "               'stop': False,\n",
       "               'continuation': None}),\n",
       "             ('step_1',\n",
       "              {'type': 'sample',\n",
       "               'name': 'step_1',\n",
       "               'fn': Uniform(low: -1.0, high: 1.0),\n",
       "               'is_observed': False,\n",
       "               'args': (),\n",
       "               'kwargs': {},\n",
       "               'value': tensor(-0.9900),\n",
       "               'infer': {},\n",
       "               'scale': 1.0,\n",
       "               'mask': None,\n",
       "               'cond_indep_stack': (),\n",
       "               'done': True,\n",
       "               'stop': False,\n",
       "               'continuation': None}),\n",
       "             ('obs',\n",
       "              {'type': 'sample',\n",
       "               'name': 'obs',\n",
       "               'fn': Normal(loc: 1.100000023841858, scale: 0.10000000149011612),\n",
       "               'is_observed': True,\n",
       "               'args': (),\n",
       "               'kwargs': {},\n",
       "               'value': tensor(1.9800),\n",
       "               'infer': {},\n",
       "               'scale': 1.0,\n",
       "               'mask': None,\n",
       "               'cond_indep_stack': (),\n",
       "               'done': True,\n",
       "               'stop': False,\n",
       "               'continuation': None}),\n",
       "             ('_RETURN',\n",
       "              {'name': '_RETURN',\n",
       "               'type': 'return',\n",
       "               'value': 1.0567030906677246})])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro_trace.nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
